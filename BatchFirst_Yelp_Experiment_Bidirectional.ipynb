{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BatchFirst_Yelp_Experiment_Bidirectional.ipynb","provenance":[],"collapsed_sections":["ej_j3Duf1NS2"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hPrtL5ka1EB5"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"4DBTCI10hQ3E"},"source":["# from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOr9FqjtPeuF"},"source":["# uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvPfmMHXhY9y","executionInfo":{"status":"ok","timestamp":1618079529314,"user_tz":240,"elapsed":5984,"user":{"displayName":"Naveen Makkar","photoUrl":"","userId":"02161116282895600612"}},"outputId":"b5332fdf-dac1-498e-e3b9-2c0b19ba71c4"},"source":["import numpy as np\n","import pandas as pd\n","import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import brown\n","nltk.download('punkt')\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from sklearn.model_selection import train_test_split\n","np.random.seed(0)\n","torch.manual_seed(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7ff1392908f0>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u2fpcj8jvV1_","executionInfo":{"status":"ok","timestamp":1618079530192,"user_tz":240,"elapsed":6791,"user":{"displayName":"Naveen Makkar","photoUrl":"","userId":"02161116282895600612"}},"outputId":"6e9eaf52-dfe8-4c7a-89f3-31b604ee45cb"},"source":["nltk.download('brown')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1TPDt-MPtN9l","executionInfo":{"status":"ok","timestamp":1618079535074,"user_tz":240,"elapsed":9330,"user":{"displayName":"Naveen Makkar","photoUrl":"","userId":"02161116282895600612"}},"outputId":"df1e2147-42b1-4b47-8fd6-1fc0eb0d1bad"},"source":["nltk.download('brown')\n","data = list(brown.sents())\n","new_data = []\n","for x in data:\n","  new_data.append(' '.join(x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nhTMLgcLsxaO"},"source":["tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(new_data)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AF0mnS84tq7M","executionInfo":{"status":"ok","timestamp":1618079626730,"user_tz":240,"elapsed":69545,"user":{"displayName":"Naveen Makkar","photoUrl":"","userId":"02161116282895600612"}},"outputId":"209836c5-4d73-42fa-ef9b-b1dcbc43be80"},"source":["max_epochs = 10\n","vec_size = 256\n","alpha = 0.025\n","\n","doc2vec_model = Doc2Vec(vector_size=vec_size,\n","                alpha=alpha, \n","                min_alpha=0.00025,\n","                min_count=1,\n","                dm =1)\n","  \n","doc2vec_model.build_vocab(tagged_data)\n","\n","for epoch in range(max_epochs):\n","    print('iteration {0}'.format(epoch))\n","    doc2vec_model.train(tagged_data,\n","                total_examples=doc2vec_model.corpus_count,\n","                epochs=doc2vec_model.iter)\n","    # decrease the learning rate\n","    doc2vec_model.alpha -= 0.0002\n","    # fix the learning rate, no decay\n","    doc2vec_model.min_alpha = doc2vec_model.alpha\n","\n","doc2vec_model.save(\"d2v.model\")\n","print(\"Model Saved\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["iteration 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"],"name":"stderr"},{"output_type":"stream","text":["Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f1-vTJT9E2Pt"},"source":["class YelpDataset(Dataset):\n","    \"\"\"Yelp dataset.\"\"\"\n","\n","    def __init__(self, file_name, append_zeros = True):\n","        \"\"\"\n","        Args:\n","            file_name: The json file to make the dataset from\n","        \"\"\"\n","        self.df = pd.read_json(file_name, lines=True)\n","\n","        tensors = []\n","        binary_cat = []\n","        zero_sentence = np.zeros(vec_size)\n","\n","        #Create target class and document vector for each review\n","        for index, row in self.df.iterrows():\n","            single_class = np.zeros(2)\n","            if row['category'] == 1:\n","              single_class[1] = 1\n","            else:\n","              single_class[0] = 1\n","            binary_cat.append(torch.tensor(single_class))\n","\n","            sentences = sent_tokenize(row['text'])\n","            sent_vecs = []\n","            for i in range(20):\n","              if i < len(sentences):\n","                sent_vecs.append(np.array(doc2vec_model.infer_vector(word_tokenize(sentences[i]))))\n","              elif append_zeros:\n","                sent_vecs.append(zero_sentence.copy())\n","            tensors.append(torch.FloatTensor(sent_vecs))\n","\n","        self.df['category'] = binary_cat\n","        self.df['vector'] = tensors\n","        del self.df['text']\n","\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        category = self.df.iloc[idx, 0]\n","        vector = self.df.iloc[idx, 1]\n","        sample = {'vector': vector, 'category': category}\n","\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hv4hy6LuLy-C"},"source":["### DO NOT APPEND ZEROS ###\n","#append_zeros = False\n","dataset_train = YelpDataset('dataset/dataset_train.json', False)\n","dataset_dev = YelpDataset('dataset/dataset_dev.json', False)\n","dataset_test = YelpDataset('dataset/dataset_test.json', False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMhEg-OcLPH2"},"source":["dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, \n","                              num_workers=0)\n","dataloader_dev = DataLoader(dataset_dev, batch_size=1, shuffle=True, \n","                              num_workers=0)\n","dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=True, \n","                              num_workers=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8wAn40qabTV"},"source":["# Updated Model"]},{"cell_type":"code","metadata":{"id":"64iE9wTXafvC"},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hD_i2TTPaqeh"},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n","\n","    def forward(self, inputs):\n","        output, hidden = self.gru(inputs)\n","        return output, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jbyh27jSa4Ws"},"source":["class BinaryClassifier(nn.Module):\n","    def __init__(self, input_size):\n","        super(BinaryClassifier, self).__init__()\n","        self.input_size = input_size\n","        \n","        self.fcn = nn.Sequential(\n","            nn.Linear(2*input_size, 10),\n","            nn.Tanh(),\n","            nn.Linear(10, 2),\n","            nn.Tanh()\n","        )\n","\n","\n","    def forward(self, x):\n","        output = self.fcn(x)\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fWiL8yQeQyc","outputId":"90af9866-d5fa-4ba2-d0e5-0eaa7d4b9a4d"},"source":["from tqdm import tqdm\n","\n","encoder = EncoderRNN(256, 32)\n","classifier = BinaryClassifier(32)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n","\n","epochs = 10\n","total = 0\n","for n in range(epochs):\n","    epoch_loss = 0\n","    for sample_batched in tqdm(dataloader_train):\n","        encoder.zero_grad()\n","        classifier.zero_grad()\n","        loss = 0\n","        seq = sample_batched['vector']\n","        output, hidden = encoder(seq)\n","        output = output[0][-1]\n","        output = classifier(output)\n","        output = output.view(1, -1)\n","        arg_max = torch.argmax(sample_batched['category'][0])\n","        target = torch.tensor([arg_max])\n","        loss += criterion(output, target)\n","        epoch_loss+=loss.detach().item()\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        classifier_optimizer.step()\n","\n","    if n:\n","        print(\"Average loss at epoch {}: {}\".format(n, epoch_loss/len(dataloader_train)))\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:08<00:00, 201.07it/s]\n","100%|██████████| 50000/50000 [04:23<00:00, 189.47it/s]\n","  0%|          | 16/50000 [00:00<05:15, 158.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 1: 0.42830004631757734\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:23<00:00, 189.85it/s]\n","  0%|          | 22/50000 [00:00<03:53, 213.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 2: 0.3935349872377515\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:23<00:00, 190.06it/s]\n","  0%|          | 19/50000 [00:00<04:36, 180.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 3: 0.36263542630195617\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:23<00:00, 189.57it/s]\n","  0%|          | 38/50000 [00:00<04:24, 189.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 4: 0.33688908943653106\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:22<00:00, 190.23it/s]\n","  0%|          | 22/50000 [00:00<03:48, 218.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 5: 0.3169146253216267\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:23<00:00, 189.83it/s]\n","  0%|          | 20/50000 [00:00<04:18, 193.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 6: 0.2982011951285601\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:23<00:00, 189.80it/s]\n","  0%|          | 37/50000 [00:00<04:36, 180.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 7: 0.28601427686482667\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:23<00:00, 189.88it/s]\n","  0%|          | 21/50000 [00:00<04:07, 201.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 8: 0.27688027049601077\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:03<00:00, 205.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 9: 0.2671962743285298\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"h_k-qBQ5OvGm","outputId":"d53f1e42-e780-45a7-9236-4870b5bec76d"},"source":["total_correct = 0\n","total = 0\n","for sample_batched in tqdm(dataloader_train):\n","\n","    loss = 0\n","    output, hidden = encoder(sample_batched['vector'])\n","\n","    output = output[0][len(output[0])-1]\n","    output = classifier(output)\n","    classification = torch.argmax(output)\n","    arg_max = torch.argmax(sample_batched['category'][0])\n","    if classification == arg_max:\n","        total_correct+=1\n","print(\"Accuracy: {}\".format(total_correct/len(dataloader_train)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [01:13<00:00, 683.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.93812\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qlp9SsbA6M8V","outputId":"e08309bc-ff52-4642-f253-09477de6493f"},"source":["total_correct = 0\n","total = 0\n","for sample_batched in tqdm(dataloader_dev):\n","\n","    loss = 0\n","    output, hidden = encoder(sample_batched['vector'])\n","    output = output[0][len(output[0])-1]\n","    output = classifier(output)\n","    classification = torch.argmax(output)\n","    arg_max = torch.argmax(sample_batched['category'][0])\n","    if classification == arg_max:\n","        total_correct+=1\n","print(\"Accuracy: {}\".format(total_correct/len(dataloader_dev)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 10000/10000 [00:14<00:00, 683.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.7971\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kPiTQv2POvGo"},"source":[""],"execution_count":null,"outputs":[]}]}