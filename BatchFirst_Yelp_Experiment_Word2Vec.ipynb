{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BatchFirst_Yelp_Experiment_Word2Vec.ipynb","provenance":[],"collapsed_sections":["ej_j3Duf1NS2"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hPrtL5ka1EB5"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"4DBTCI10hQ3E"},"source":["# from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOr9FqjtPeuF"},"source":["# uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvPfmMHXhY9y","outputId":"bda95fe5-0cf8-4932-865a-5e3ce2311d5e"},"source":["import numpy as np\n","import pandas as pd\n","import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import brown\n","nltk.download('punkt')\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from gensim.models.word2vec import Word2Vec\n","from sklearn.model_selection import train_test_split\n","import logging\n","\n","np.random.seed(0)\n","torch.manual_seed(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/bharatsuri/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x14719da90>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u2fpcj8jvV1_","outputId":"3ff4c508-01ad-4675-b6f1-5c448d057b34"},"source":["nltk.download('brown')\n","data = brown.sents()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to\n","[nltk_data]     /Users/bharatsuri/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AF0mnS84tq7M","outputId":"1686df28-f1c0-4469-9659-04e034a23090"},"source":["max_epochs = 10\n","vec_size = 256\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","word2vec_model = Word2Vec(data, min_count = 1, size = vec_size, window = 6, iter = max_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-03-23 23:26:20,726 : INFO : collecting all words and their counts\n","2021-03-23 23:26:20,728 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2021-03-23 23:26:21,059 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n","2021-03-23 23:26:21,360 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n","2021-03-23 23:26:21,688 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n","2021-03-23 23:26:21,995 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n","2021-03-23 23:26:22,228 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n","2021-03-23 23:26:22,412 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n","2021-03-23 23:26:22,413 : INFO : Loading a fresh vocabulary\n","2021-03-23 23:26:22,513 : INFO : effective_min_count=1 retains 56057 unique words (100% of original 56057, drops 0)\n","2021-03-23 23:26:22,513 : INFO : effective_min_count=1 leaves 1161192 word corpus (100% of original 1161192, drops 0)\n","2021-03-23 23:26:22,594 : INFO : deleting the raw counts dictionary of 56057 items\n","2021-03-23 23:26:22,595 : INFO : sample=0.001 downsamples 38 most-common words\n","2021-03-23 23:26:22,595 : INFO : downsampling leaves estimated 854152 word corpus (73.6% of prior 1161192)\n","2021-03-23 23:26:22,664 : INFO : estimated required memory for 56057 words and 256 dimensions: 142833236 bytes\n","2021-03-23 23:26:22,664 : INFO : resetting layer weights\n","2021-03-23 23:26:29,610 : INFO : training model with 3 workers on 56057 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n","2021-03-23 23:26:30,615 : INFO : EPOCH 1 - PROGRESS: at 47.04% examples, 439353 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:31,531 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:31,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:31,545 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:31,545 : INFO : EPOCH - 1 : training on 1161192 raw words (853490 effective words) took 1.9s, 441498 effective words/s\n","2021-03-23 23:26:32,549 : INFO : EPOCH 2 - PROGRESS: at 47.04% examples, 439593 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:33,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:33,480 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:33,487 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:33,488 : INFO : EPOCH - 2 : training on 1161192 raw words (853791 effective words) took 1.9s, 439747 effective words/s\n","2021-03-23 23:26:34,498 : INFO : EPOCH 3 - PROGRESS: at 47.85% examples, 444857 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:35,419 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:35,424 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:35,432 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:35,433 : INFO : EPOCH - 3 : training on 1161192 raw words (854478 effective words) took 1.9s, 439595 effective words/s\n","2021-03-23 23:26:36,441 : INFO : EPOCH 4 - PROGRESS: at 47.85% examples, 445181 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:37,358 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:37,363 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:37,372 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:37,372 : INFO : EPOCH - 4 : training on 1161192 raw words (853693 effective words) took 1.9s, 440510 effective words/s\n","2021-03-23 23:26:38,383 : INFO : EPOCH 5 - PROGRESS: at 47.04% examples, 437037 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:39,342 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:39,347 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:39,355 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:39,355 : INFO : EPOCH - 5 : training on 1161192 raw words (854498 effective words) took 2.0s, 431153 effective words/s\n","2021-03-23 23:26:40,371 : INFO : EPOCH 6 - PROGRESS: at 47.04% examples, 434636 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:41,329 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:41,335 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:41,342 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:41,342 : INFO : EPOCH - 6 : training on 1161192 raw words (854074 effective words) took 2.0s, 430056 effective words/s\n","2021-03-23 23:26:42,363 : INFO : EPOCH 7 - PROGRESS: at 45.19% examples, 411607 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:43,375 : INFO : EPOCH 7 - PROGRESS: at 98.03% examples, 412329 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:43,400 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:43,405 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:43,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:43,413 : INFO : EPOCH - 7 : training on 1161192 raw words (853587 effective words) took 2.1s, 412718 effective words/s\n","2021-03-23 23:26:44,419 : INFO : EPOCH 8 - PROGRESS: at 45.19% examples, 417804 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:45,421 : INFO : EPOCH 8 - PROGRESS: at 96.96% examples, 413767 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:45,462 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:45,468 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:45,474 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:45,475 : INFO : EPOCH - 8 : training on 1161192 raw words (854082 effective words) took 2.1s, 414644 effective words/s\n","2021-03-23 23:26:46,495 : INFO : EPOCH 9 - PROGRESS: at 45.87% examples, 418822 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:47,505 : INFO : EPOCH 9 - PROGRESS: at 98.86% examples, 416536 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:47,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:47,517 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:47,524 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:47,524 : INFO : EPOCH - 9 : training on 1161192 raw words (854123 effective words) took 2.0s, 417152 effective words/s\n","2021-03-23 23:26:48,541 : INFO : EPOCH 10 - PROGRESS: at 45.87% examples, 420526 words/s, in_qsize 0, out_qsize 0\n","2021-03-23 23:26:49,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2021-03-23 23:26:49,548 : INFO : EPOCH 10 - PROGRESS: at 99.06% examples, 419006 words/s, in_qsize 1, out_qsize 1\n","2021-03-23 23:26:49,548 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2021-03-23 23:26:49,554 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2021-03-23 23:26:49,555 : INFO : EPOCH - 10 : training on 1161192 raw words (854058 effective words) took 2.0s, 421113 effective words/s\n","2021-03-23 23:26:49,555 : INFO : training on a 11611920 raw words (8539874 effective words) took 19.9s, 428193 effective words/s\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-ELXnLjwSn1p"},"source":["  def getSentenceVector(sentence):\n","    words = word_tokenize(sentence)\n","    count_present = 0\n","    vec = np.zeros(256)\n","    for word in words:\n","      if word in word2vec_model.wv:\n","        vec = np.add(vec, np.array(word2vec_model.wv[word]))\n","        count_present += 1\n","    if count_present > 0:\n","      vec = vec / count_present\n","    return vec\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1-vTJT9E2Pt"},"source":["class YelpDataset(Dataset):\n","    \"\"\"Yelp dataset.\"\"\"\n","    \n","    def __init__(self, file_name, append_zeros = True):\n","        \"\"\"\n","        Args:\n","            file_name: The json file to make the dataset from\n","        \"\"\"\n","        self.df = pd.read_json(file_name, lines=True)\n","\n","        tensors = []\n","        binary_cat = []\n","        zero_sentence = np.zeros(vec_size)\n","\n","        #Create target class and document vector for each review\n","        for index, row in self.df.iterrows():\n","            single_class = np.zeros(2)\n","            if row['category'] == 1:\n","              single_class[1] = 1\n","            else:\n","              single_class[0] = 1\n","            binary_cat.append(torch.tensor(single_class))\n","\n","            sentences = sent_tokenize(row['text'])\n","            sent_vecs = []\n","            for i in range(20):\n","              if i < len(sentences):\n","                sent_vecs.append(getSentenceVector(sentences[i]))\n","              elif append_zeros:\n","                sent_vecs.append(zero_sentence.copy())\n","            tensors.append(torch.FloatTensor(sent_vecs))\n","\n","        self.df['category'] = binary_cat\n","        self.df['vector'] = tensors\n","        del self.df['text']\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        category = self.df.iloc[idx, 0]\n","        vector = self.df.iloc[idx, 1]\n","        sample = {'vector': vector, 'category': category}\n","\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hv4hy6LuLy-C"},"source":["### DO NOT APPEND ZEROS ###\n","#append_zeros = False\n","dataset_train = YelpDataset('dataset/dataset_train.json', False)\n","dataset_dev = YelpDataset('dataset/dataset_dev.json', False)\n","dataset_test = YelpDataset('dataset/dataset_test.json', False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMhEg-OcLPH2"},"source":["dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, \n","                              num_workers=0)\n","dataloader_dev = DataLoader(dataset_dev, batch_size=1, shuffle=True, \n","                              num_workers=0)\n","dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=True, \n","                              num_workers=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"64iE9wTXafvC"},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hD_i2TTPaqeh"},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n","\n","    def forward(self, inputs):\n","        output, hidden = self.gru(inputs)\n","        return output, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jbyh27jSa4Ws"},"source":["class BinaryClassifier(nn.Module):\n","    def __init__(self, input_size):\n","        super(BinaryClassifier, self).__init__()\n","        self.input_size = input_size\n","        \n","        self.fcn = nn.Sequential(\n","            nn.Linear(2*input_size, 10),\n","            nn.Tanh(),\n","            nn.Linear(10, 2),\n","            nn.Tanh()\n","        )\n","\n","\n","    def forward(self, x):\n","        output = self.fcn(x)\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fWiL8yQeQyc","outputId":"27e27559-edad-47b2-ec9d-71e09432fe51"},"source":["from tqdm import tqdm\n","encoder = EncoderRNN(256, 32)\n","classifier = BinaryClassifier(32)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n","\n","epochs = 10\n","total = 0\n","for n in range(epochs):\n","    epoch_loss = 0\n","    for sample_batched in tqdm(dataloader_train):\n","        encoder.zero_grad()\n","        classifier.zero_grad()\n","        loss = 0\n","        seq = sample_batched['vector']\n","        output, hidden = encoder(seq)\n","        output = output[0][-1]\n","        output = classifier(output)\n","        output = output.view(1, -1)\n","        arg_max = torch.argmax(sample_batched['category'][0])\n","        target = torch.tensor([arg_max])\n","        loss += criterion(output, target)\n","        epoch_loss+=loss.detach().item()\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        classifier_optimizer.step()\n","\n","    if n:\n","        print(\"Average loss at epoch {}: {}\".format(n, epoch_loss/len(dataloader_train)))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [03:58<00:00, 210.00it/s]\n","100%|██████████| 50000/50000 [04:01<00:00, 206.63it/s]\n","  0%|          | 16/50000 [00:00<05:21, 155.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 1: 0.48865831643253566\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:21<00:00, 191.21it/s]\n","  0%|          | 22/50000 [00:00<03:52, 215.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 2: 0.4792340424075723\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:22<00:00, 190.84it/s]\n","  0%|          | 18/50000 [00:00<04:49, 172.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 3: 0.4735036727231741\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:21<00:00, 191.46it/s]\n","  0%|          | 39/50000 [00:00<04:17, 194.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 4: 0.46673577694654467\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:21<00:00, 191.38it/s]\n","  0%|          | 23/50000 [00:00<03:37, 229.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 5: 0.4649085663637519\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:22<00:00, 190.73it/s]\n","  0%|          | 40/50000 [00:00<04:17, 193.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 6: 0.46238606805980204\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:21<00:00, 191.01it/s]\n","  0%|          | 38/50000 [00:00<04:27, 186.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 7: 0.4603581267696619\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:21<00:00, 191.13it/s]\n","  0%|          | 21/50000 [00:00<04:02, 205.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 8: 0.45822789230793715\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:21<00:00, 191.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 9: 0.4581519177404046\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"KvHygU4QPCTe","outputId":"79c08296-0d73-436a-f575-53357280d945"},"source":["total_correct = 0\n","total = 0\n","for sample_batched in tqdm(dataloader_train):\n","\n","    loss = 0\n","    output, hidden = encoder(sample_batched['vector'])\n","\n","    output = output[0][len(output[0])-1]\n","    output = classifier(output)\n","    classification = torch.argmax(output)\n","    arg_max = torch.argmax(sample_batched['category'][0])\n","    if classification == arg_max:\n","        total_correct+=1\n","print(\"Accuracy: {}\".format(total_correct/len(dataloader_train)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [01:18<00:00, 636.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.79314\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qlp9SsbA6M8V","outputId":"3232416b-bb38-4554-ce1f-f993fac14a45"},"source":["total_correct = 0\n","total = 0\n","for sample_batched in tqdm(dataloader_dev):\n","\n","    loss = 0\n","    output, hidden = encoder(sample_batched['vector'])\n","\n","    output = output[0][len(output[0])-1]\n","    output = classifier(output)\n","    classification = torch.argmax(output)\n","    arg_max = torch.argmax(sample_batched['category'][0])\n","    if classification == arg_max:\n","        total_correct+=1\n","print(\"Accuracy: {}\".format(total_correct/len(dataloader_dev)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 10000/10000 [00:15<00:00, 633.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.7907\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"N5JnxspiPCTe"},"source":[""],"execution_count":null,"outputs":[]}]}