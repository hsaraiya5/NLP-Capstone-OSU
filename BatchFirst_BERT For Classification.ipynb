{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"colab":{"name":"BatchFirst_BERT For Classification.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"imported-density"},"source":["import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n","from nltk.tokenize import sent_tokenize\n","from tqdm import tqdm\n","\n","import torch.optim as optim\n","\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","import logging\n","logging.basicConfig(level=logging.INFO)"],"id":"imported-density","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"broad-visibility"},"source":["train_texts = []\n","train_labels = []\n","with open('dataset/dataset_train.json', 'r') as file:\n","    for line in file:\n","        j = json.loads(line)\n","        train_texts.append(j['text'])\n","        train_labels.append(j['category'])\n","train_labels = list(map(lambda x: torch.tensor([x]), train_labels))\n","\n","dev_texts = []\n","dev_labels = []\n","with open('dataset/dataset_dev.json', 'r') as file:\n","    for line in file:\n","        j = json.loads(line)\n","        dev_texts.append(j['text'])\n","        dev_labels.append(j['category'])\n","dev_labels = list(map(lambda x: torch.tensor([x]), dev_labels))"],"id":"broad-visibility","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mechanical-prophet"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n","model.eval()\n","\n","MAX_SEQ_LEN = 128\n","PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n","UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"],"id":"mechanical-prophet","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"allied-volleyball","outputId":"79c7620d-d208-41e0-b843-1faa6bb2497e"},"source":["train_data = []\n","for review in tqdm(train_texts):\n","    sents = sent_tokenize(review)\n","    embeddings = []\n","    for sent in sents:\n","        marked_text = \"[CLS] \" + sent + \" [SEP]\"\n","        tokenized_text = tokenizer.tokenize(marked_text)\n","        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","        segment_ids = [1] * len(tokenized_text)\n","        \n","        tokens_tensor = torch.tensor([indexed_tokens])\n","        segments_tensor = torch.tensor([segment_ids])\n","        \n","        with torch.no_grad():\n","            outputs = model(tokens_tensor, segments_tensor)\n","            hidden_states = outputs[2]\n","            token_embeddings = torch.stack(hidden_states, dim=0)\n","            token_embeddings = torch.squeeze(token_embeddings, dim=1)[-2]\n","            \n","            sentence_embedding = torch.mean(token_embeddings, dim=0).view(1, 1, -1)\n","            embeddings.append(sentence_embedding)\n","    train_data.append(torch.cat(embeddings, dim=1))\n","\n","    \n","dev_data = []\n","for review in tqdm(dev_texts):\n","    sents = sent_tokenize(review)\n","    embeddings = []\n","    for sent in sents:\n","        marked_text = \"[CLS] \" + sent + \" [SEP]\"\n","        tokenized_text = tokenizer.tokenize(marked_text)\n","        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","        segment_ids = [1] * len(tokenized_text)\n","        \n","        tokens_tensor = torch.tensor([indexed_tokens])\n","        segments_tensor = torch.tensor([segment_ids])\n","        \n","        with torch.no_grad():\n","            outputs = model(tokens_tensor, segments_tensor)\n","            hidden_states = outputs[2]\n","            token_embeddings = torch.stack(hidden_states, dim=0)\n","            token_embeddings = torch.squeeze(token_embeddings, dim=1)[-2]\n","            \n","            sentence_embedding = torch.mean(token_embeddings, dim=0).view(1, 1, -1)\n","            embeddings.append(sentence_embedding)\n","    dev_data.append(torch.cat(embeddings, dim=1))"],"id":"allied-volleyball","execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [7:00:53<00:00,  1.98it/s]   \n","100%|██████████| 10000/10000 [1:24:23<00:00,  1.97it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"medieval-small"},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size, hidden_size, bidirectional=True, batch_first=True)\n","\n","    def forward(self, inputs):\n","        output, hidden = self.gru(inputs)\n","        return output, hidden"],"id":"medieval-small","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"random-suspect"},"source":["class BinaryClassifier(nn.Module):\n","    def __init__(self, input_size):\n","        super(BinaryClassifier, self).__init__()\n","\n","        self.input_size = input_size\n","        \n","        self.fcn = nn.Sequential(\n","            nn.Linear(2*input_size, 10),\n","            nn.Tanh(),\n","            nn.Linear(10, 2),\n","            nn.Tanh()\n","        )\n","\n","\n","    def forward(self, x):\n","        output = self.fcn(x)\n","        \n","        return output"],"id":"random-suspect","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"unable-discrimination","outputId":"4141fc3f-13f3-48f2-8ab9-3f25ad6d3083"},"source":["encoder = EncoderRNN(768, 32)\n","classifier = BinaryClassifier(32)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n","\n","idx = np.array(range(len(train_data)))\n","np.random.shuffle(idx)\n","\n","epochs = 10\n","total = 0\n","for n in range(epochs):\n","    epoch_loss = 0\n","    for i in tqdm(idx):\n","        x, y = train_data[i], train_labels[i]\n","\n","        encoder.zero_grad()\n","        classifier.zero_grad()\n","\n","        loss = 0\n","        output, hidden = encoder(x)\n","        output = output[0][-1]\n","\n","        output = classifier(output)\n","        output = output.view(1, -1)\n","        loss = criterion(output, y)\n","        epoch_loss+=loss.detach().item()\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        classifier_optimizer.step()\n","    \n","    print(\"Average loss at epoch {}: {}\".format(n, epoch_loss/len(train_data)))\n","\n","\n"],"id":"unable-discrimination","execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:14<00:00, 196.43it/s]\n","  0%|          | 23/50000 [00:00<03:40, 226.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 0: 0.2929267506468296\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:04<00:00, 204.40it/s]\n","  0%|          | 21/50000 [00:00<04:01, 206.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 1: 0.2724859392657876\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:12<00:00, 198.29it/s]\n","  0%|          | 19/50000 [00:00<04:35, 181.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 2: 0.268146974298954\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:49<00:00, 172.94it/s]\n","  0%|          | 14/50000 [00:00<06:00, 138.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 3: 0.26612275010883807\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [05:21<00:00, 155.63it/s]\n","  0%|          | 18/50000 [00:00<04:42, 176.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 4: 0.2634788509759307\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [05:05<00:00, 163.74it/s]\n","  0%|          | 13/50000 [00:00<06:29, 128.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 5: 0.259367814719975\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [05:33<00:00, 150.05it/s]\n","  0%|          | 15/50000 [00:00<05:40, 146.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 6: 0.26084127724915745\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [05:34<00:00, 149.40it/s]\n","  0%|          | 16/50000 [00:00<05:13, 159.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 7: 0.255678585421741\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [05:34<00:00, 149.33it/s]\n","  0%|          | 30/50000 [00:00<05:41, 146.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 8: 0.25458301654368637\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [05:35<00:00, 149.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 9: 0.2553473915401101\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"correct-offset","outputId":"d52300f4-7261-42ff-bb1f-3495a670967a"},"source":["encoder.eval()\n","classifier.eval()\n","with torch.no_grad():\n","    total_correct = 0\n","    total = 0\n","    for x, y in zip(train_data, train_labels):\n","        output, hidden = encoder(x)\n","\n","        output = output[0][-1]\n","        output = classifier(output)\n","        classification = torch.argmax(output)\n","        if classification.item() == y.item():\n","            total_correct+=1\n","print(\"Accuracy: {}\".format(total_correct/len(train_data)))"],"id":"correct-offset","execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.92916\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"acute-messenger","outputId":"431fcc1e-de8c-44e3-d734-2cedb4606e79"},"source":["encoder.eval()\n","classifier.eval()\n","with torch.no_grad():\n","    total_correct = 0\n","    total = 0\n","    for x, y in zip(dev_data, dev_labels):\n","        output, hidden = encoder(x)\n","\n","        output = output[0][-1]\n","        output = classifier(output)\n","        classification = torch.argmax(output)\n","        if classification.item() == y.item():\n","            total_correct+=1\n","print(\"Accuracy: {}\".format(total_correct/len(dev_data)))"],"id":"acute-messenger","execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.9149\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"placed-integration"},"source":["import pickle\n","\n","torch.save({'train_data': train_data, 'train_labels': train_labels}, 'train_data.pt')\n","torch.save({'dev_data': dev_data, 'dev_labels': dev_labels}, 'dev_data.pt')\n","torch.save(encoder.state_dict(), 'encoder_with_BERT')\n","torch.save(classifier.state_dict(), 'classifier_with_BERT')"],"id":"placed-integration","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"comparable-option"},"source":[""],"id":"comparable-option","execution_count":null,"outputs":[]}]}