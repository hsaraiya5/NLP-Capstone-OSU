{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Basic classifier with Attention","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KJDWYEE4NiPX"},"source":["from __future__ import unicode_literals, print_function, division\r\n","from io import open\r\n","import unicodedata\r\n","import string\r\n","import re\r\n","import random\r\n","\r\n","import torch\r\n","import torch.nn as nn\r\n","from torch import optim\r\n","import torch.nn.functional as F\r\n","\r\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vF1ic8TqX3HV"},"source":["class RnnType:\r\n","    GRU = 1\r\n","    LSTM = 2\r\n","\r\n","class AttentionModel:\r\n","    NONE = 0\r\n","    DOT = 1\r\n","    GENERAL = 2\r\n","    CONCAT = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZicnw6CNqiQ"},"source":["class Attention(nn.Module):\r\n","  \r\n","    def __init__(self, device, method, hidden_size):\r\n","        super(Attention, self).__init__()\r\n","        self.device = device\r\n","\r\n","        self.method = method\r\n","        self.hidden_size = hidden_size\r\n","\r\n","        self.concat_linear = nn.Linear(self.hidden_size * 2, self.hidden_size)\r\n","\r\n","        if self.method == AttentionModel.GENERAL:\r\n","            self.attn = nn.Linear(self.hidden_size, hidden_size)\r\n","\r\n","        elif self.method == AttentionModel.CONCAT:\r\n","            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\r\n","            self.other = torch.FloatTensor(1, hidden_size)\r\n","\r\n","    def forward(self, rnn_outputs, final_hidden_state):\r\n","        # rnn_output.shape:         (batch_size, seq_len, hidden_size)\r\n","        # final_hidden_state.shape: (batch_size, hidden_size)\r\n","        # NOTE: hidden_size may also reflect bidirectional hidden states (hidden_size = num_directions * hidden_dim)\r\n","        batch_size, seq_len, _ = rnn_outputs.shape\r\n","        if self.method == AttentionModel.DOT:\r\n","            attn_weights = torch.bmm(rnn_outputs, final_hidden_state.unsqueeze(2))\r\n","        elif self.method == AttentionModel.GENERAL:\r\n","            attn_weights = self.attn(rnn_outputs) # (batch_size, seq_len, hidden_dim)\r\n","            attn_weights = torch.bmm(attn_weights, final_hidden_state.unsqueeze(2))\r\n","        #elif self.method == AttentionModel.CONCAT:\r\n","        #    NOT IMPLEMENTED\r\n","        else:\r\n","            raise Exception(\"[Error] Unknown AttentionModel.\")\r\n","\r\n","        attn_weights = F.softmax(attn_weights.squeeze(2), dim=1)\r\n","\r\n","        context = torch.bmm(rnn_outputs.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)\r\n","\r\n","        attn_hidden = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state), dim=1)))\r\n","\r\n","        return attn_hidden, attn_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkuAiR5ROBpe"},"source":["class RnnClassifier(nn.Module):\r\n","  \r\n","    def __init__(self, device, params):\r\n","        super(RnnClassifier, self).__init__()\r\n","        self.params = params\r\n","        self.device = device\r\n","\r\n","        # Embedding layer\r\n","        # self.word_embeddings = nn.Embedding(self.params['vocab_size'], self.params['embed_dim'])\r\n","\r\n","        # Calculate number of directions\r\n","        self.num_directions = 1\r\n","\r\n","        self.linear_dims = [self.params['rnn_hidden_dim'] * self.num_directions] + self.params['linear_dims']\r\n","        self.linear_dims.append(self.params['label_size'])\r\n","\r\n","        # RNN layer\r\n","        rnn = nn.LSTM\r\n","        self.rnn = rnn(self.params['embed_dim'],\r\n","                       self.params['rnn_hidden_dim'],\r\n","                       num_layers=self.params['num_layers'],\r\n","                       bidirectional=self.params['bidirectional'],\r\n","                       dropout=self.params['dropout'],\r\n","                       batch_first=False)\r\n","\r\n","\r\n","        # Define set of fully connected layers (Linear Layer + Activation Layer) * #layers\r\n","        self.linears = nn.ModuleList()\r\n","        for i in range(0, len(self.linear_dims)-1):\r\n","            if self.params['dropout'] > 0.0:\r\n","                self.linears.append(nn.Dropout(p=self.params['dropout']))\r\n","            linear_layer = nn.Linear(self.linear_dims[i], self.linear_dims[i+1])\r\n","            self.init_weights(linear_layer)\r\n","            self.linears.append(linear_layer)\r\n","            if i == len(self.linear_dims) - 1:\r\n","                break  # no activation after output layer!!!\r\n","            self.linears.append(nn.ReLU())\r\n","\r\n","        self.hidden = None\r\n","\r\n","        # Choose attention model\r\n","        if self.params['attention_model'] != AttentionModel.NONE:\r\n","            self.attn = Attention(self.device, self.params['attention_model'], self.params['rnn_hidden_dim'] * self.num_directions)\r\n","\r\n","\r\n","\r\n","    def init_hidden(self, batch_size):\r\n","        return (torch.zeros(self.params['num_layers'] * self.num_directions, batch_size, self.params['rnn_hidden_dim']).to(self.device),\r\n","                torch.zeros(self.params['num_layers'] * self.num_directions, batch_size, self.params['rnn_hidden_dim']).to(self.device))\r\n","\r\n","\r\n","    def freeze_layer(self, layer):\r\n","        for param in layer.parameters():\r\n","            param.requires_grad = False\r\n","\r\n","\r\n","    def forward(self, inputs):\r\n","        batch_size, seq_len = inputs.shape\r\n","\r\n","        # Push through embedding layer\r\n","        # X = self.word_embeddings(inputs).transpose(0, 1)\r\n","\r\n","        X = inputs.transpose(0,1)\r\n","\r\n","        # Push through RNN layer\r\n","        rnn_output, self.hidden = self.rnn(X, self.hidden)\r\n","\r\n","        # Extract last hidden state\r\n","        final_state = self.hidden[0].view(self.params['num_layers'], self.num_directions, batch_size, self.params['rnn_hidden_dim'])[-1]\r\n","        # Handle directions\r\n","        final_hidden_state = final_state.squeeze(0)\r\n","\r\n","        # Push through attention layer\r\n","        attn_weights = None\r\n","        if self.params['attention_model'] != AttentionModel.NONE:\r\n","            rnn_output = rnn_output.permute(1, 0, 2)  #\r\n","            X, attn_weights = self.attn(rnn_output, final_hidden_state)\r\n","        else:\r\n","            X = final_hidden_state\r\n","\r\n","        # Push through linear layers\r\n","        for l in self.linears:\r\n","            X = l(X)\r\n","\r\n","        log_probs = F.log_softmax(X, dim=1)\r\n","\r\n","        return log_probs, attn_weights\r\n","\r\n","\r\n","    def init_weights(self, layer):\r\n","        if type(layer) == nn.Linear:\r\n","            print(\"Initialize layer with nn.init.xavier_uniform_: {}\".format(layer))\r\n","            torch.nn.init.xavier_uniform_(layer.weight)\r\n","            layer.bias.data.fill_(0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rFJe4ZYBOcCE"},"source":["def train(input_tensor, target_tensor, rrn_classifier, rnn_classifier_optimizer, criterion, max_length=500):\r\n","    \r\n","    rnn_classifier_hidden = rnn_classifier.initHidden(batch_size = 1)\r\n","\r\n","    rnn_classifier_optimizer.zero_grad()\r\n","\r\n","\r\n","    input_length = input_tensor.size(0)\r\n","    target_length = target_tensor.size(0)\r\n","\r\n","    rnn_classifier_outputs = torch.zeros(max_length, rnn_classifier.hidden_size, device=device)\r\n","\r\n","    loss = 0\r\n","\r\n","    for ei in range(input_length):\r\n","        rnn_classifier_output, rnn_classifier_hidden = rnn_classifier(\r\n","            input_tensor[ei], rnn_classifier_hidden)\r\n","        rnn_classifier_outputs[ei] = rnn_classifier_output[0, 0]\r\n","\r\n","\r\n","    # Without teacher forcing: use its own predictions as the next input\r\n","    for di in range(target_length):\r\n","        loss += criterion(rnn_classifier_output, target_tensor[di])\r\n","\r\n","    loss.backward()\r\n","\r\n","    rnn_classifier_optimizer.step()\r\n"," \r\n","    return loss.item() / target_length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IEoB3lKERTIT"},"source":["\r\n","def trainIters(rnn_classifier, inputs, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\r\n","    start = time.time()\r\n","    plot_losses = []\r\n","    print_loss_total = 0  # Reset every print_every\r\n","    plot_loss_total = 0  # Reset every plot_every\r\n","\r\n","    rnn_classifier_optimizer = optim.SGD(rnn_classifier.parameters(), lr=learning_rate)\r\n","    training_pairs = inputs\r\n","    criterion = nn.NLLLoss()\r\n","\r\n","    for iter in range(1, n_iters + 1):\r\n","        training_pair = training_pairs[iter - 1]\r\n","        input_tensor = training_pair[0]\r\n","        target_tensor = training_pair[1]\r\n","\r\n","        loss = train(input_tensor, target_tensor, rnn_classifier, rnn_classifier_optimizer, criterion)\r\n","        print_loss_total += loss\r\n","        plot_loss_total += loss\r\n","\r\n","        if iter % print_every == 0:\r\n","            print_loss_avg = print_loss_total / print_every\r\n","            print_loss_total = 0\r\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\r\n","                                         iter, iter / n_iters * 100, print_loss_avg))\r\n","\r\n","        if iter % plot_every == 0:\r\n","            plot_loss_avg = plot_loss_total / plot_every\r\n","            plot_losses.append(plot_loss_avg)\r\n","            plot_loss_total = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AfuSkcIKR_N9"},"source":["def evaluate(rnn_classifier, sentence, max_length=500):\r\n","    with torch.no_grad():\r\n","        input_tensor = sentence\r\n","        input_length = input_tensor.size()[0]\r\n","        rnn_classifier_hidden = rnn_classifier.initHidden()\r\n","\r\n","        rnn_classifier_outputs = torch.zeros(max_length, rnn_classifier.hidden_size, device=device)\r\n","\r\n","        for ei in range(input_length):\r\n","            rnn_classifier_output, rnn_classifier_hidden = rnn_classifier(input_tensor[ei],\r\n","                                                     rnn_classifier_hidden)\r\n","            rnn_classifier_outputs[ei] += rnn_classifier_output[0, 0]\r\n","\r\n","\r\n","        return rnn_classifier_outputs, rnn_classifiers_hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"Z5R-tU7iS4FC","executionInfo":{"status":"error","timestamp":1615065842108,"user_tz":300,"elapsed":298,"user":{"displayName":"Hrishikesh Saraiya","photoUrl":"","userId":"13186101804767105633"}},"outputId":"4bbe685b-cb5b-4c77-fb21-080ee57a8e0e"},"source":["\r\n","params={'vocab_size': 1000,\r\n","        'embed_dim': 256,\r\n","        'rnn_hidden_dim': 256 ,\r\n","        'num_layers': 2,\r\n","        'attention_model': AttentionModel.DOT ,\r\n","        'linear_dims': [256],\r\n","        'label_size': 1,\r\n","        'bidirectional': 0,\r\n","        'dropout': 0.1}\r\n","\r\n","rnn_classifier_1 = RnnClassifier(device, params)\r\n","\r\n","trainIters(rnn_classifier_1, inputs,  75000, print_every=5000)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Initialize layer with nn.init.xavier_uniform_: Linear(in_features=256, out_features=256, bias=True)\n","Initialize layer with nn.init.xavier_uniform_: Linear(in_features=256, out_features=1, bias=True)\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-0a7a4349f996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrnn_classifier_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRnnClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_classifier_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: trainIters() missing 1 required positional argument: 'n_iters'"]}]}]}