{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BatchFirst_Yelp_Experiment_GloVe.ipynb","provenance":[],"collapsed_sections":["ej_j3Duf1NS2"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hPrtL5ka1EB5"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"4DBTCI10hQ3E"},"source":["# from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOr9FqjtPeuF"},"source":["# uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvPfmMHXhY9y","outputId":"bda95fe5-0cf8-4932-865a-5e3ce2311d5e"},"source":["import numpy as np\n","import pandas as pd\n","import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import brown\n","nltk.download('punkt')\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from gensim.models.word2vec import Word2Vec\n","from sklearn.model_selection import train_test_split\n","import gensim\n","import logging\n","from tqdm import tqdm\n","\n","np.random.seed(0)\n","torch.manual_seed(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/bharatsuri/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x148fdba90>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AF0mnS84tq7M","outputId":"1686df28-f1c0-4469-9659-04e034a23090"},"source":["logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","model = gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format('glove.42B.300d.w2vformat.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-03-23 23:26:42,371 : INFO : loading projection weights from glove.42B.300d.w2vformat.txt\n","2021-03-23 23:30:31,619 : INFO : loaded (1917494, 300) matrix from glove.42B.300d.w2vformat.txt\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-ELXnLjwSn1p"},"source":["def getSentenceVector(sentence):\n","    words = word_tokenize(sentence)\n","    count_present = 0\n","    vec = np.zeros(300)\n","    for word in words:\n","        if word in model.wv:\n","            vec = np.add(vec, np.array(model.wv[word]))\n","            count_present += 1\n","    if count_present > 0:\n","        vec = vec / count_present\n","    return vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1-vTJT9E2Pt"},"source":["class YelpDataset(Dataset):\n","    \"\"\"Yelp dataset.\"\"\"\n","    \n","    def __init__(self, file_name, append_zeros = True):\n","        \"\"\"\n","        Args:\n","            file_name: The json file to make the dataset from\n","        \"\"\"\n","        self.df = pd.read_json(file_name, lines=True)\n","\n","        tensors = []\n","        binary_cat = []\n","        zero_sentence = np.zeros(300)\n","\n","        #Create target class and document vector for each review\n","        for index, row in self.df.iterrows():\n","            single_class = np.zeros(2)\n","            if row['category'] == 1:\n","              single_class[1] = 1\n","            else:\n","              single_class[0] = 1\n","            binary_cat.append(torch.tensor(single_class))\n","\n","            sentences = sent_tokenize(row['text'])\n","            sent_vecs = []\n","            for i in range(20):\n","              if i < len(sentences):\n","                sent_vecs.append(getSentenceVector(sentences[i]))\n","              elif append_zeros:\n","                sent_vecs.append(zero_sentence.copy())\n","            tensors.append(torch.FloatTensor(sent_vecs))\n","\n","        self.df['category'] = binary_cat\n","        self.df['vector'] = tensors\n","        del self.df['text']\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        category = self.df.iloc[idx, 0]\n","        vector = self.df.iloc[idx, 1]\n","        sample = {'vector': vector, 'category': category}\n","\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hv4hy6LuLy-C","outputId":"7eaad926-89bd-47f0-d0b6-ae1924caaa86"},"source":["### DO NOT APPEND ZEROS ###\n","#append_zeros = False\n","dataset_train = YelpDataset('dataset/dataset_train.json', False)\n","dataset_dev = YelpDataset('dataset/dataset_dev.json', False)\n","dataset_test = YelpDataset('dataset/dataset_test.json', False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<ipython-input-5-9dc5e6bd6841>:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  if word in model.wv:\n","<ipython-input-5-9dc5e6bd6841>:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  vec = np.add(vec, np.array(model.wv[word]))\n","<ipython-input-5-9dc5e6bd6841>:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  if word in model.wv:\n","<ipython-input-5-9dc5e6bd6841>:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  vec = np.add(vec, np.array(model.wv[word]))\n","<ipython-input-5-9dc5e6bd6841>:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  if word in model.wv:\n","<ipython-input-5-9dc5e6bd6841>:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  vec = np.add(vec, np.array(model.wv[word]))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"KMhEg-OcLPH2"},"source":["dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, \n","                              num_workers=0)\n","dataloader_dev = DataLoader(dataset_dev, batch_size=1, shuffle=True, \n","                              num_workers=0)\n","dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=True, \n","                              num_workers=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"64iE9wTXafvC"},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hD_i2TTPaqeh"},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n","\n","    def forward(self, inputs):\n","        output, hidden = self.gru(inputs)\n","        return output, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jbyh27jSa4Ws"},"source":["class BinaryClassifier(nn.Module):\n","    def __init__(self, input_size):\n","        super(BinaryClassifier, self).__init__()\n","        self.input_size = input_size\n","        \n","        self.fcn = nn.Sequential(\n","            nn.Linear(2*input_size, 10),\n","            nn.Tanh(),\n","            nn.Linear(10, 2),\n","            nn.Tanh()\n","        )\n","\n","\n","    def forward(self, x):\n","        output = self.fcn(x)\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fWiL8yQeQyc","outputId":"27e27559-edad-47b2-ec9d-71e09432fe51"},"source":["from tqdm import tqdm\n","encoder = EncoderRNN(300, 32)\n","classifier = BinaryClassifier(32)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n","\n","epochs = 10\n","total = 0\n","for n in range(epochs):\n","    epoch_loss = 0\n","    for sample_batched in tqdm(dataloader_train):\n","        encoder.zero_grad()\n","        classifier.zero_grad()\n","        loss = 0\n","        seq = sample_batched['vector']\n","        output, hidden = encoder(seq)\n","        output = output[0][-1]\n","        output = classifier(output)\n","        output = output.view(1, -1)\n","        arg_max = torch.argmax(sample_batched['category'][0])\n","        target = torch.tensor([arg_max])\n","        loss += criterion(output, target)\n","        epoch_loss+=loss.detach().item()\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        classifier_optimizer.step()\n","\n","    if n:\n","        print(\"Average loss at epoch {}: {}\".format(n, epoch_loss/len(dataloader_train)))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:25<00:00, 188.26it/s]\n","100%|██████████| 50000/50000 [04:25<00:00, 188.66it/s]\n","  0%|          | 20/50000 [00:00<04:16, 195.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 1: 0.31197062472462656\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:24<00:00, 189.22it/s]\n","  0%|          | 20/50000 [00:00<04:20, 191.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 2: 0.2977838070911169\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:24<00:00, 189.22it/s]\n","  0%|          | 36/50000 [00:00<04:38, 179.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 3: 0.2898059875065088\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:24<00:00, 188.72it/s]\n","  0%|          | 19/50000 [00:00<04:32, 183.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 4: 0.2811680551958084\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:24<00:00, 188.93it/s]\n","  0%|          | 18/50000 [00:00<04:41, 177.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 5: 0.2762741073349118\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:24<00:00, 189.13it/s]\n","  0%|          | 18/50000 [00:00<04:42, 176.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 6: 0.26877209519535306\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:24<00:00, 189.16it/s]\n","  0%|          | 18/50000 [00:00<04:41, 177.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 7: 0.2640499986863136\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [04:09<00:00, 200.05it/s]\n","  0%|          | 42/50000 [00:00<03:58, 209.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 8: 0.2601344144052267\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 50000/50000 [03:53<00:00, 214.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Average loss at epoch 9: 0.2549951467335224\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4VdqZEPF1Qsl","outputId":"c22e4412-5cc9-4ae6-a274-cbb07bb7656f"},"source":["total_correct = 0\n","total = 0\n","for sample_batched in tqdm(dataloader_train):\n","\n","    loss = 0\n","    output, hidden = encoder(sample_batched['vector'])\n","\n","    output = output[0][len(output[0])-1]\n","    output = classifier(output)\n","    classification = torch.argmax(output)\n","    arg_max = torch.argmax(sample_batched['category'][0])\n","    if classification == arg_max:\n","        total_correct+=1\n","print(\"Accuracy: {}\".format(total_correct/len(dataloader_train)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [01:08<00:00, 734.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.93504\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qlp9SsbA6M8V","outputId":"aaa79bb2-62d6-408b-9874-4d455a2acbd6"},"source":["total_correct = 0\n","total = 0\n","for sample_batched in tqdm(dataloader_dev):\n","\n","    loss = 0\n","    output, hidden = encoder(sample_batched['vector'])\n","\n","    output = output[0][len(output[0])-1]\n","    output = classifier(output)\n","    classification = torch.argmax(output)\n","    arg_max = torch.argmax(sample_batched['category'][0])\n","    if classification == arg_max:\n","        total_correct+=1\n","print(\"Accuracy: {}\".format(total_correct/len(dataloader_dev)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 10000/10000 [00:13<00:00, 731.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy: 0.8989\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2ZvyIhS61Qsn"},"source":[""],"execution_count":null,"outputs":[]}]}